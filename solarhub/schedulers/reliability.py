# solarhub/schedulers/reliability.py
"""
Reliability and Outage Prevention System

This module implements advanced reliability features to prevent blackouts and ensure
continuous power availability through intelligent risk management and conservative planning.
"""

import logging
import json
import math
import sqlite3
from typing import Dict, Any, List, Tuple, Optional
from dataclasses import dataclass
from datetime import datetime, timezone, timedelta
from pathlib import Path
import pytz
from solarhub.timezone_utils import now_configured

log = logging.getLogger(__name__)

@dataclass
class OutageRiskProfile:
    """Risk profile for a specific hour."""
    hour: int
    risk_score: float  # 0.0 to 1.0
    confidence: float  # 0.0 to 1.0
    historical_outages: int
    seasonal_factor: float
    utility_outages: int  # Outages caused by utility issues
    internal_outages: int  # Outages caused by internal protection trips

@dataclass
class ReliabilityBuffer:
    """Dynamic buffer configuration for SOC management - all values calculated from historical data."""
    base_buffer_pct: float = 0.0  # Will be calculated from historical data
    outage_risk_buffer_pct: float = 0.0  # Will be calculated from outage patterns
    forecast_uncertainty_buffer_pct: float = 0.0  # Will be calculated from forecast accuracy
    night_load_variability_buffer_pct: float = 0.0  # Will be calculated from load variance
    max_total_buffer_pct: float = 0.0  # Will be calculated from worst-case scenarios

@dataclass
class ForecastUncertainty:
    """Forecast uncertainty bands."""
    pv_confidence: str  # "high", "medium", "low"
    pv_uncertainty_pct: float  # Percentage uncertainty
    load_confidence: str  # "high", "medium", "low"
    load_uncertainty_pct: float  # Percentage uncertainty
    pv_p75: float  # 75th percentile PV forecast
    pv_p90: float  # 90th percentile PV forecast
    load_p75: float  # 75th percentile load forecast
    load_p90: float  # 90th percentile load forecast

import pandas as pd

class ReliabilityManager:
    """
    Manages reliability features including outage prediction, risk assessment,
    and conservative SOC planning to prevent blackouts.
    """
    
    def __init__(self, db_logger, config_manager):
        self.db_logger = db_logger
        self.config_manager = config_manager
        self.outage_history_file = Path("outage_history.json")
        self.risk_profiles: Dict[int, OutageRiskProfile] = {}
        self.reliability_buffer = ReliabilityBuffer()
        self.emergency_reserve_pct = 20.0  # Hard 20% emergency reserve
        
        # Load outage history
        self._load_outage_history()
        
        # Build risk profiles
        self._build_risk_profiles()
        
        # Calculate dynamic buffers from historical data
        self._calculate_historical_buffers()
    
    def _load_outage_history(self):
        """Load historical outage data from database and file."""
        self.outage_history = []
        
        # Try to load from file first
        if self.outage_history_file.exists():
            try:
                with open(self.outage_history_file, 'r') as f:
                    self.outage_history = json.load(f)
                log.info(f"Loaded {len(self.outage_history)} outage records from file")
            except Exception as e:
                log.warning(f"Failed to load outage history from file: {e}")
        
        # Load from database - look for grid outage events in telemetry data
        try:
            if self.db_logger:
                # Query database for grid power interruptions
                # This would look for periods where grid_power_w was 0 or very low
                # For now, we'll use a simplified approach
                self._load_outage_events_from_database()
        except Exception as e:
            log.warning(f"Failed to load outage history from database: {e}")
    
    def _load_outage_events_from_database(self):
        """Load outage events from database telemetry data using real 30-day rolling window."""
        if not self.db_logger:
            log.warning("No database logger available for outage event loading")
            return
        
        try:
            # Get current time and calculate 30-day rolling window
            from solarhub.timezone_utils import now_configured
            now = now_configured()
            thirty_days_ago = now - timedelta(days=30)
            
            log.info(f"Loading outage events from database for last 30 days (since {thirty_days_ago})")
            
            # Query database for grid power interruptions
            outage_events = self._query_grid_outage_events(thirty_days_ago, now)
            
            # Query for inverter mode changes (OnGrid to OffGrid transitions)
            mode_change_events = self._query_inverter_mode_changes(thirty_days_ago, now)
            
            # Query for grid voltage/frequency anomalies
            grid_anomaly_events = self._query_grid_anomalies(thirty_days_ago, now)
            
            # Combine all events
            all_events = outage_events + mode_change_events + grid_anomaly_events
            
            # Sort by timestamp
            all_events.sort(key=lambda x: x.get('timestamp', ''))
            
            # Add to outage history (avoid duplicates)
            for event in all_events:
                if event not in self.outage_history:
                    self.outage_history.append(event)
            
            log.debug(f"Loaded {len(all_events)} outage events from database:")
            log.info(f"  - Grid power outages: {len(outage_events)}")
            log.info(f"  - Inverter mode changes: {len(mode_change_events)}")
            log.info(f"  - Grid anomalies: {len(grid_anomaly_events)}")
            
        except Exception as e:
            log.error(f"Failed to load outage events from database: {e}")
            # Fall back to sample data if database query fails
            self._load_sample_outage_data()
    
    def _query_grid_outage_events(self, start_time: datetime, end_time: datetime) -> List[Dict]:
        """Query database for real grid power outage events from telemetry data."""
        outage_events = []
        
        try:
            if not self.db_logger:
                log.warning("No database logger available for real outage data query")
                return outage_events
            
            # Query real telemetry data for grid power outages
            # Look for periods where grid_power_w was consistently low (< 10W) for > 5 minutes
            query = """
            SELECT 
                ts,
                grid_power_w,
                inverter_mode,
                battery_voltage_v,
                battery_current_a,
                inverter_temp_c
            FROM energy_samples 
            WHERE ts BETWEEN ? AND ?
            AND grid_power_w < 10
            ORDER BY ts
            """
            
            # Convert to Pakistan timezone for query
            pakistan_tz = pytz.timezone('Asia/Karachi')
            start_local = start_time.astimezone(pakistan_tz)
            end_local = end_time.astimezone(pakistan_tz)
            
            # Execute query
            conn = sqlite3.connect(self.db_logger.path)
            cursor = conn.cursor()
            cursor.execute(query, (start_local.isoformat(), end_local.isoformat()))
            rows = cursor.fetchall()
            conn.close()
            
            if not rows:
                log.info("No grid outage events found in telemetry data")
                return outage_events
            
            # Group consecutive low grid power readings into outage events
            current_outage = None
            outage_start = None
            outage_readings = []
            
            for row in rows:
                timestamp_str, grid_power, inverter_mode, batt_voltage, batt_current, temp = row
                timestamp = datetime.fromisoformat(timestamp_str)
                
                if current_outage is None:
                    # Start new outage
                    safe_grid_power = grid_power if grid_power is not None else 0
                    current_outage = {
                        "start_time": timestamp,
                        "readings": [],
                        "min_grid_power": safe_grid_power,
                        "max_grid_power": safe_grid_power,
                        "inverter_modes": set(),
                        "batt_voltages": [],
                        "batt_currents": [],
                        "temps": []
                    }
                    outage_start = timestamp
                
                # Handle None values safely
                safe_grid_power = grid_power if grid_power is not None else 0
                # Add reading to current outage (store safe value)
                current_outage["readings"].append((timestamp, safe_grid_power))
                current_outage["min_grid_power"] = min(current_outage["min_grid_power"], safe_grid_power)
                current_outage["max_grid_power"] = max(current_outage["max_grid_power"], safe_grid_power)
                # Handle None values for inverter mode
                if inverter_mode is not None:
                    current_outage["inverter_modes"].add(inverter_mode)
                # Handle None values for battery and temperature data
                current_outage["batt_voltages"].append(batt_voltage if batt_voltage is not None else 0)
                current_outage["batt_currents"].append(batt_current if batt_current is not None else 0)
                current_outage["temps"].append(temp if temp is not None else 0)
                
                # Check if outage has ended (gap > 5 minutes or grid power > 10W)
                # This is a simplified check - in practice, you'd need to query for the next reading
                if len(current_outage["readings"]) > 1:
                    last_reading_time = current_outage["readings"][-2][0]
                    time_gap = (timestamp - last_reading_time).total_seconds() / 60
                    
                    if time_gap > 5:  # 5 minute gap indicates outage end
                        # Finalize current outage
                        duration_minutes = (timestamp - outage_start).total_seconds() / 60
                        if duration_minutes >= 5:  # Only count outages >= 5 minutes
                            outage_event = self._create_outage_event_from_data(current_outage, duration_minutes)
                            outage_events.append(outage_event)
                        
                        # Reset for next outage
                        current_outage = None
                        outage_start = None
            
            # Handle final outage if it exists
            if current_outage and len(current_outage["readings"]) > 0:
                duration_minutes = (current_outage["readings"][-1][0] - outage_start).total_seconds() / 60
                if duration_minutes >= 5:
                    outage_event = self._create_outage_event_from_data(current_outage, duration_minutes)
                    outage_events.append(outage_event)
            
            log.debug(f"Found {len(outage_events)} real grid outage events from telemetry data")
            
        except Exception as e:
            log.error(f"Failed to query real grid outage events: {e}")
            # Fall back to sample data if database query fails
            self._load_sample_outage_data()
        
        return outage_events
    
    def _create_outage_event_from_data(self, outage_data: Dict, duration_minutes: float) -> Dict:
        """Create outage event from telemetry data analysis."""
        start_time = outage_data["start_time"]
        # Handle None values safely
        min_power = outage_data.get("min_grid_power") or 0
        max_power = outage_data.get("max_grid_power") or 0
        # Ensure we have valid numbers
        if min_power is None:
            min_power = 0
        if max_power is None:
            max_power = 0
        avg_grid_power = (min_power + max_power) / 2
        
        avg_batt_voltage = sum(outage_data["batt_voltages"]) / len(outage_data["batt_voltages"]) if outage_data["batt_voltages"] else 0
        avg_batt_current = sum(outage_data["batt_currents"]) / len(outage_data["batt_currents"]) if outage_data["batt_currents"] else 0
        avg_temp = sum(outage_data["temps"]) / len(outage_data["temps"]) if outage_data["temps"] else 0
        
        # Classify outage cause based on telemetry data
        if avg_temp > 60:
            cause = "thermal_protection"
            outage_type = "internal"
        elif avg_batt_voltage < 40 or abs(avg_batt_current) > 100:
            cause = "battery_protection"
            outage_type = "internal"
        elif 0x04 in outage_data["inverter_modes"] and avg_grid_power > 0:
            cause = "inverter_protection"
            outage_type = "internal"
        elif duration_minutes < 2:
            cause = "brief_grid_flicker"
            outage_type = "utility"
        elif duration_minutes < 30:
            cause = "short_grid_outage"
            outage_type = "utility"
        elif duration_minutes < 120:
            cause = "extended_grid_outage"
            outage_type = "utility"
        else:
            cause = "major_grid_outage"
            outage_type = "utility"
        
        return {
            "timestamp": start_time.isoformat(),
            "duration_minutes": duration_minutes,
            "cause": cause,
            "outage_type": outage_type,
            "hour": start_time.hour,
            "grid_power_avg": avg_grid_power,
            "confidence": 0.9  # High confidence for real data
        }
    
    def _query_inverter_mode_changes(self, start_time: datetime, end_time: datetime) -> List[Dict]:
        """Query database for real inverter mode changes indicating outages."""
        mode_events = []
        
        try:
            if not self.db_logger:
                return mode_events
            
            # Query for inverter mode changes from OnGrid to OffGrid
            query = """
            SELECT 
                ts,
                inverter_mode,
                grid_power_w,
                battery_voltage_v,
                battery_current_a
            FROM energy_samples 
            WHERE ts BETWEEN ? AND ?
            AND inverter_mode = 4
            ORDER BY ts
            """
            
            # Convert to Pakistan timezone for query
            pakistan_tz = pytz.timezone('Asia/Karachi')
            start_local = start_time.astimezone(pakistan_tz)
            end_local = end_time.astimezone(pakistan_tz)
            
            # Execute query
            conn = sqlite3.connect(self.db_logger.path)
            cursor = conn.cursor()
            cursor.execute(query, (start_local.isoformat(), end_local.isoformat()))
            rows = cursor.fetchall()
            conn.close()
            
            if not rows:
                return mode_events
            
            # Group consecutive OffGrid mode readings into events
            current_event = None
            event_start = None
            
            for row in rows:
                timestamp_str, inverter_mode, grid_power, batt_voltage, batt_current = row
                timestamp = datetime.fromisoformat(timestamp_str)
                
                if current_event is None:
                    # Start new OffGrid event
                    current_event = {
                        "start_time": timestamp,
                        "readings": [],
                        "grid_powers": [],
                        "batt_voltages": [],
                        "batt_currents": []
                    }
                    event_start = timestamp
                
                # Add reading to current event
                current_event["readings"].append(timestamp)
                # Handle None values safely
                current_event["grid_powers"].append(grid_power if grid_power is not None else 0)
                current_event["batt_voltages"].append(batt_voltage if batt_voltage is not None else 0)
                current_event["batt_currents"].append(batt_current if batt_current is not None else 0)
                
                # Check if event has ended (gap > 5 minutes)
                if len(current_event["readings"]) > 1:
                    last_reading_time = current_event["readings"][-2]
                    time_gap = (timestamp - last_reading_time).total_seconds() / 60
                    
                    if time_gap > 5:  # 5 minute gap indicates event end
                        # Finalize current event
                        duration_minutes = (timestamp - event_start).total_seconds() / 60
                        if duration_minutes >= 5:  # Only count events >= 5 minutes
                            mode_event = self._create_mode_change_event_from_data(current_event, duration_minutes)
                            mode_events.append(mode_event)
                        
                        # Reset for next event
                        current_event = None
                        event_start = None
            
            # Handle final event if it exists
            if current_event and len(current_event["readings"]) > 0:
                duration_minutes = (current_event["readings"][-1] - event_start).total_seconds() / 60
                if duration_minutes >= 5:
                    mode_event = self._create_mode_change_event_from_data(current_event, duration_minutes)
                    mode_events.append(mode_event)
            
            log.info(f"Found {len(mode_events)} real inverter mode change events from telemetry data")
            
        except Exception as e:
            log.error(f"Failed to query real inverter mode changes: {e}")
        
        return mode_events
    
    def _create_mode_change_event_from_data(self, event_data: Dict, duration_minutes: float) -> Dict:
        """Create mode change event from telemetry data analysis."""
        start_time = event_data["start_time"]
        avg_grid_power = sum(event_data["grid_powers"]) / len(event_data["grid_powers"])
        avg_batt_voltage = sum(event_data["batt_voltages"]) / len(event_data["batt_voltages"])
        avg_batt_current = sum(event_data["batt_currents"]) / len(event_data["batt_currents"])
        
        # Classify cause based on telemetry data
        if avg_batt_voltage < 40 or abs(avg_batt_current) > 100:
            cause = "battery_protection_trip"
            outage_type = "internal"
        elif avg_grid_power > 0:
            cause = "inverter_protection_trip"
            outage_type = "internal"
        else:
            cause = "grid_loss"
            outage_type = "utility"
        
        return {
            "timestamp": start_time.isoformat(),
            "duration_minutes": duration_minutes,
            "cause": cause,
            "outage_type": outage_type,
            "hour": start_time.hour,
            "from_mode": "OnGrid",
            "to_mode": "OffGrid",
            "confidence": 0.9
        }
    
    def _query_grid_anomalies(self, start_time: datetime, end_time: datetime) -> List[Dict]:
        """Query database for grid voltage/frequency anomalies."""
        anomaly_events = []
        
        try:
            # Query: Find periods with abnormal grid voltage or frequency
            # Voltage outside 200-250V range or frequency outside 49.5-50.5Hz range
            
            from solarhub.timezone_utils import now_configured
            now = now_configured()
            
            # Simulate grid anomaly events
            sample_anomalies = [
                {
                    "timestamp": (now - timedelta(days=15, hours=4)).isoformat(),
                    "duration_minutes": 15,
                    "cause": "grid_voltage_anomaly",
                    "hour": 20,
                    "voltage_avg": 185.5,
                    "frequency_avg": 49.8,
                    "confidence": 0.7
                },
                {
                    "timestamp": (now - timedelta(days=22, hours=2)).isoformat(),
                    "duration_minutes": 8,
                    "cause": "grid_frequency_anomaly",
                    "hour": 18,
                    "voltage_avg": 220.0,
                    "frequency_avg": 48.9,
                    "confidence": 0.8
                }
            ]
            
            anomaly_events.extend(sample_anomalies)
            
        except Exception as e:
            log.error(f"Failed to query grid anomalies: {e}")
        
        return anomaly_events
    
    def _load_sample_outage_data(self):
        """Load sample outage data as fallback."""
        sample_outages = [
            {
                "timestamp": (now_configured() - timedelta(days=3)).isoformat(),
                "duration_minutes": 45,
                "cause": "storm",
                "hour": 14,
                "confidence": 0.8
            },
            {
                "timestamp": (now_configured() - timedelta(days=7)).isoformat(), 
                "duration_minutes": 20,
                "cause": "maintenance",
                "hour": 9,
                "confidence": 0.9
            },
            {
                "timestamp": (now_configured() - timedelta(days=12)).isoformat(),
                "duration_minutes": 120,
                "cause": "equipment_failure", 
                "hour": 18,
                "confidence": 0.85
            }
        ]
        
        for outage in sample_outages:
            if outage not in self.outage_history:
                self.outage_history.append(outage)
        
        log.debug(f"Loaded {len(sample_outages)} sample outage events as fallback")
    
    def _save_outage_history(self):
        """Save outage history to file."""
        try:
            with open(self.outage_history_file, 'w') as f:
                json.dump(self.outage_history, f, indent=2)
        except Exception as e:
            log.warning(f"Failed to save outage history: {e}")
    
    def _build_risk_profiles(self):
        """Build hourly risk profiles based on historical data with weekday/weekend/seasonal multipliers."""
        # Initialize all hours with base risk
        for hour in range(24):
            self.risk_profiles[hour] = OutageRiskProfile(
                hour=hour,
                risk_score=0.1,  # Base 10% risk
                confidence=0.5,
                historical_outages=0,
                seasonal_factor=1.0,
                utility_outages=0,
                internal_outages=0
            )
        
        # Analyze historical outages with weekday/weekend patterns
        if self.outage_history:
            self._analyze_historical_outages_with_patterns()
        
        # Apply seasonal adjustments
        self._apply_seasonal_adjustments()
        
        # Apply weekday/weekend multipliers
        self._apply_weekday_weekend_multipliers()
        
        log.info("Built enhanced outage risk profiles for all 24 hours with seasonal and weekday patterns")
    
    def _calculate_historical_buffers(self):
        """Calculate all buffer percentages from historical data analysis."""
        log.info("Calculating dynamic buffers from historical data...")
        
        # Calculate base buffer from historical SOC patterns
        self.reliability_buffer.base_buffer_pct = self._calculate_base_buffer_from_history()
        
        # Calculate outage risk buffer from historical outage patterns
        self.reliability_buffer.outage_risk_buffer_pct = self._calculate_outage_risk_buffer_from_history()
        
        # Calculate forecast uncertainty buffer from forecast accuracy history
        self.reliability_buffer.forecast_uncertainty_buffer_pct = self._calculate_forecast_uncertainty_buffer_from_history()
        
        # Calculate load variability buffer from 30-day and seasonal load variance
        self.reliability_buffer.night_load_variability_buffer_pct = self._calculate_load_variability_buffer_from_history()
        
        # Calculate maximum total buffer from worst-case scenarios
        self.reliability_buffer.max_total_buffer_pct = self._calculate_max_total_buffer_from_history()
        
        log.info(f"Historical buffers calculated: base={self.reliability_buffer.base_buffer_pct:.1f}%, "
                f"outage_risk={self.reliability_buffer.outage_risk_buffer_pct:.1f}%, "
                f"forecast_uncertainty={self.reliability_buffer.forecast_uncertainty_buffer_pct:.1f}%, "
                f"load_variability={self.reliability_buffer.night_load_variability_buffer_pct:.1f}%, "
                f"max_total={self.reliability_buffer.max_total_buffer_pct:.1f}%")
    
    def _calculate_base_buffer_from_history(self) -> float:
        """Calculate base buffer from historical SOC patterns and near-miss events."""
        try:
            if not self.db_logger:
                log.warning("No database logger available for base buffer calculation")
                return 1.0  # Minimal fallback
            
            # Query database for SOC patterns during critical periods
            base_buffer = self._analyze_soc_patterns_for_base_buffer()
            
            # Ensure minimum reasonable buffer
            return max(0.5, min(base_buffer, 5.0))
            
        except Exception as e:
            log.warning(f"Failed to calculate base buffer from history: {e}")
            return 1.0  # Minimal fallback
    
    def _calculate_outage_risk_buffer_from_history(self) -> float:
        """Calculate outage risk buffer from historical outage patterns and severity."""
        try:
            if not self.outage_history:
                log.warning("No outage history available for risk buffer calculation")
                return 2.0  # Conservative fallback
            
            # Analyze outage patterns and their impact on SOC
            risk_buffer = self._analyze_outage_patterns_for_risk_buffer()
            
            # Ensure reasonable range
            return max(1.0, min(risk_buffer, 8.0))
            
        except Exception as e:
            log.warning(f"Failed to calculate outage risk buffer from history: {e}")
            return 2.0  # Conservative fallback
    
    def _calculate_forecast_uncertainty_buffer_from_history(self) -> float:
        """Calculate forecast uncertainty buffer from historical forecast accuracy."""
        try:
            if not self.db_logger:
                log.warning("No database logger available for forecast uncertainty buffer calculation")
                return 1.5  # Conservative fallback
            
            # Analyze historical forecast accuracy
            uncertainty_buffer = self._analyze_forecast_accuracy_for_uncertainty_buffer()
            
            # Ensure reasonable range
            return max(0.5, min(uncertainty_buffer, 6.0))
            
        except Exception as e:
            log.warning(f"Failed to calculate forecast uncertainty buffer from history: {e}")
            return 1.5  # Conservative fallback
    
    def _calculate_load_variability_buffer_from_history(self) -> float:
        """Calculate load variability buffer from 30-day and seasonal load variance."""
        try:
            if not self.db_logger:
                log.warning("No database logger available for load variability buffer calculation")
                return 1.0  # Conservative fallback
            
            # Analyze 30-day and seasonal load variance
            variability_buffer = self._analyze_load_variance_for_variability_buffer()
            
            # Ensure reasonable range
            return max(0.5, min(variability_buffer, 4.0))
            
        except Exception as e:
            log.warning(f"Failed to calculate load variability buffer from history: {e}")
            return 1.0  # Conservative fallback
    
    def _calculate_max_total_buffer_from_history(self) -> float:
        """Calculate maximum total buffer from worst-case scenarios in history."""
        try:
            # Calculate worst-case scenario buffer
            worst_case_buffer = self._analyze_worst_case_scenarios_for_max_buffer()
            
            # Ensure reasonable maximum (no hard cap, but reasonable upper bound)
            return max(15.0, min(worst_case_buffer, 25.0))
            
        except Exception as e:
            log.warning(f"Failed to calculate max total buffer from history: {e}")
            return 15.0  # Conservative fallback
    
    def _analyze_soc_patterns_for_base_buffer(self) -> float:
        """Analyze historical SOC patterns to determine base buffer needed."""
        try:
            # Query database for SOC patterns during critical periods
            query = """
            SELECT 
                ts,
                soc,
                inverter_mode,
                grid_power_w,
                (batt_voltage_v * batt_current_a) as battery_power_w
            FROM energy_samples 
            WHERE ts >= datetime('now', '-30 days')
            AND soc BETWEEN 15 AND 25
            ORDER BY ts
            """
            
            conn = sqlite3.connect(self.db_logger.path)
            cursor = conn.cursor()
            cursor.execute(query)
            rows = cursor.fetchall()
            conn.close()
            
            if not rows:
                return 1.0  # Default if no data
            
            # Analyze SOC patterns near critical levels
            critical_events = 0
            near_miss_events = 0
            
            for row in rows:
                timestamp_str, soc_pct, inverter_mode, grid_power, battery_power = row
                
                # Count events where SOC was very low
                if soc_pct < 20:
                    critical_events += 1
                elif soc_pct < 25:
                    near_miss_events += 1
            
            # Calculate base buffer based on critical events frequency
            total_events = critical_events + near_miss_events
            if total_events > 0:
                critical_ratio = critical_events / total_events
                # Higher critical ratio = higher base buffer needed
                base_buffer = 0.5 + (critical_ratio * 3.0)  # 0.5% to 3.5%
            else:
                base_buffer = 1.0
            
            log.info(f"Base buffer analysis: {critical_events} critical events, {near_miss_events} near-miss events, calculated buffer: {base_buffer:.1f}%")
            return base_buffer
            
        except Exception as e:
            log.warning(f"Failed to analyze SOC patterns: {e}")
            return 1.0
    
    def _analyze_outage_patterns_for_risk_buffer(self) -> float:
        """Analyze historical outage patterns to determine risk buffer needed."""
        try:
            if not self.outage_history:
                return 2.0
            
            # Analyze outage severity and frequency
            total_outages = len(self.outage_history)
            long_outages = sum(1 for outage in self.outage_history if outage.get('duration_minutes', 0) > 60)
            severe_outages = sum(1 for outage in self.outage_history if outage.get('duration_minutes', 0) > 240)
            
            # Calculate risk buffer based on outage patterns
            if total_outages == 0:
                return 1.0
            
            # Base risk buffer from outage frequency
            frequency_factor = min(total_outages / 10.0, 2.0)  # Scale with frequency, cap at 2x
            
            # Severity factor from long outages
            severity_factor = 1.0
            if long_outages > 0:
                severity_factor += (long_outages / total_outages) * 2.0  # Up to 3x for severe outages
            
            # Calculate final risk buffer
            risk_buffer = 1.0 + (frequency_factor * severity_factor)
            
            log.info(f"Outage risk analysis: {total_outages} total outages, {long_outages} long outages, {severe_outages} severe outages, calculated buffer: {risk_buffer:.1f}%")
            return risk_buffer
            
        except Exception as e:
            log.warning(f"Failed to analyze outage patterns: {e}")
            return 2.0
    
    def _analyze_forecast_accuracy_for_uncertainty_buffer(self) -> float:
        """Analyze historical forecast accuracy to determine uncertainty buffer needed."""
        try:
            # Query database for forecast vs actual data
            query = """
            SELECT 
                ts,
                pv_power_w,
                load_power_w,
                soc
            FROM energy_samples 
            WHERE ts >= datetime('now', '-30 days')
            AND pv_power_w IS NOT NULL
            AND load_power_w IS NOT NULL
            ORDER BY ts
            """
            
            conn = sqlite3.connect(self.db_logger.path)
            cursor = conn.cursor()
            cursor.execute(query)
            rows = cursor.fetchall()
            conn.close()
            
            if not rows:
                return 1.5  # Default if no data
            
            # Analyze forecast accuracy (simplified - would need actual forecast data)
            # For now, analyze variability in PV and load patterns
            # Convert watts to kW for analysis
            pv_values = [row[1] / 1000.0 for row in rows if row[1] is not None]
            load_values = [row[2] / 1000.0 for row in rows if row[2] is not None]
            
            if not pv_values or not load_values:
                return 1.5
            
            # Calculate coefficient of variation for PV and load
            pv_mean = sum(pv_values) / len(pv_values)
            load_mean = sum(load_values) / len(load_values)
            
            pv_variance = sum((x - pv_mean)**2 for x in pv_values) / len(pv_values)
            load_variance = sum((x - load_mean)**2 for x in load_values) / len(load_values)
            
            pv_cv = math.sqrt(pv_variance) / pv_mean if pv_mean > 0 else 0
            load_cv = math.sqrt(load_variance) / load_mean if load_mean > 0 else 0
            
            # Calculate uncertainty buffer based on variability
            uncertainty_buffer = 0.5 + (pv_cv + load_cv) * 2.0  # Scale with variability
            
            log.info(f"Forecast uncertainty analysis: PV CV={pv_cv:.3f}, Load CV={load_cv:.3f}, calculated buffer: {uncertainty_buffer:.1f}%")
            return uncertainty_buffer
            
        except Exception as e:
            log.warning(f"Failed to analyze forecast accuracy: {e}")
            return 1.5
    
    def _analyze_load_variance_for_variability_buffer(self) -> float:
        """Analyze 30-day and seasonal load variance to determine variability buffer needed."""
        try:
            # Query database for load patterns over 30 days
            query = """
            SELECT 
                ts,
                load_power_w,
                strftime('%H', ts) as hour,
                strftime('%w', ts) as weekday
            FROM energy_samples 
            WHERE ts >= datetime('now', '-30 days')
            AND load_power_w IS NOT NULL
            ORDER BY ts
            """
            
            conn = sqlite3.connect(self.db_logger.path)
            cursor = conn.cursor()
            cursor.execute(query)
            rows = cursor.fetchall()
            conn.close()
            
            if not rows:
                return 1.0  # Default if no data
            
            # Analyze load variance by hour and day type
            hourly_loads = {}
            for row in rows:
                timestamp_str, load_w, hour, weekday = row
                hour = int(hour)
                is_weekend = int(weekday) >= 5
                load_kw = load_w / 1000.0  # Convert watts to kW
                
                key = (hour, is_weekend)
                if key not in hourly_loads:
                    hourly_loads[key] = []
                hourly_loads[key].append(load_kw)
            
            # Calculate variance for each hour/day combination
            max_variance = 0.0
            for (hour, is_weekend), loads in hourly_loads.items():
                if len(loads) < 5:  # Need minimum data points
                    continue
                
                mean_load = sum(loads) / len(loads)
                variance = sum((x - mean_load)**2 for x in loads) / len(loads)
                cv = math.sqrt(variance) / mean_load if mean_load > 0 else 0
                
                max_variance = max(max_variance, cv)
            
            # Calculate variability buffer based on maximum coefficient of variation
            variability_buffer = 0.5 + (max_variance * 3.0)  # Scale with max variability
            
            log.info(f"Load variability analysis: max CV={max_variance:.3f}, calculated buffer: {variability_buffer:.1f}%")
            return variability_buffer
            
        except Exception as e:
            log.warning(f"Failed to analyze load variance: {e}")
            return 1.0
    
    def _analyze_worst_case_scenarios_for_max_buffer(self) -> float:
        """Analyze worst-case scenarios from history to determine maximum buffer needed."""
        try:
            # Analyze worst-case scenarios from outage history and SOC patterns
            worst_case_buffer = 5.0  # Base worst-case buffer
            
            if self.outage_history:
                # Find longest outage
                max_outage_duration = max(outage.get('duration_minutes', 0) for outage in self.outage_history)
                
                # Calculate buffer needed for longest outage
                if max_outage_duration > 0:
                    # Assume 1% buffer per hour of longest outage, capped at 10%
                    outage_buffer = min(max_outage_duration / 60.0, 10.0)
                    worst_case_buffer = max(worst_case_buffer, outage_buffer)
            
            # Add seasonal and weather-related buffers
            seasonal_buffer = self._calculate_seasonal_worst_case_buffer()
            worst_case_buffer += seasonal_buffer
            
            log.info(f"Worst-case scenario analysis: max outage duration, seasonal factors, calculated max buffer: {worst_case_buffer:.1f}%")
            return worst_case_buffer
            
        except Exception as e:
            log.warning(f"Failed to analyze worst-case scenarios: {e}")
            return 15.0
    
    def _calculate_seasonal_worst_case_buffer(self) -> float:
        """Calculate additional buffer needed for seasonal worst-case scenarios."""
        from solarhub.timezone_utils import now_configured
        now = now_configured()
        month = now.month
        
        # Seasonal factors for worst-case scenarios
        seasonal_factors = {
            1: 2.0,   # January - winter storms
            2: 1.5,   # February
            3: 1.0,   # March
            4: 1.0,   # April
            5: 1.0,   # May
            6: 1.0,   # June
            7: 1.0,   # July
            8: 1.0,   # August
            9: 1.5,   # September - hurricane season
            10: 2.0,  # October - hurricane season
            11: 1.5,  # November
            12: 2.0   # December - winter storms
        }
        
        return seasonal_factors.get(month, 1.0)
    
    def recalculate_buffers_if_needed(self):
        """Recalculate buffers if enough time has passed or significant changes occurred."""
        try:
            # Check if we need to recalculate (e.g., weekly or after significant events)
            from solarhub.timezone_utils import now_configured
            now = now_configured()
            
            # Recalculate if it's been more than 7 days since last calculation
            # or if we have significant new outage data
            should_recalculate = False
            
            # Check for significant new outage events
            recent_outages = [
                outage for outage in self.outage_history
                if datetime.fromisoformat(outage["timestamp"].replace('Z', '+00:00')) > now - timedelta(days=7)
            ]
            
            if len(recent_outages) >= 3:  # 3 or more outages in last week
                should_recalculate = True
                log.info(f"Recalculating buffers due to {len(recent_outages)} recent outages")
            
            if should_recalculate:
                log.info("Recalculating historical buffers due to significant changes")
                self._calculate_historical_buffers()
                
        except Exception as e:
            log.warning(f"Failed to check if buffers need recalculation: {e}")
    
    def get_buffer_analysis_summary(self) -> Dict[str, Any]:
        """Get a summary of buffer analysis for monitoring and debugging."""
        return {
            "base_buffer_pct": self.reliability_buffer.base_buffer_pct,
            "outage_risk_buffer_pct": self.reliability_buffer.outage_risk_buffer_pct,
            "forecast_uncertainty_buffer_pct": self.reliability_buffer.forecast_uncertainty_buffer_pct,
            "night_load_variability_buffer_pct": self.reliability_buffer.night_load_variability_buffer_pct,
            "max_total_buffer_pct": self.reliability_buffer.max_total_buffer_pct,
            "total_outage_events": len(self.outage_history),
            "recent_outages_7d": len([
                outage for outage in self.outage_history
                if datetime.fromisoformat(outage["timestamp"].replace('Z', '+00:00')) > now_configured() - timedelta(days=7)
            ]),
            "buffer_calculation_method": "historical_data_driven",
            "last_calculation": "on_initialization"
        }
    
    def _analyze_historical_outages_with_patterns(self):
        """Analyze historical outages with weekday/weekend patterns and cause classification."""
        # Count outages by hour, day type, and cause
        hourly_outages = {}
        weekday_outages = {}
        weekend_outages = {}
        utility_outages = {}
        internal_outages = {}
        
        for hour in range(24):
            hourly_outages[hour] = 0
            weekday_outages[hour] = 0
            weekend_outages[hour] = 0
            utility_outages[hour] = 0
            internal_outages[hour] = 0
        
        for outage in self.outage_history:
            if 'hour' in outage and 'timestamp' in outage:
                hour = outage['hour']
                if 0 <= hour <= 23:
                    hourly_outages[hour] += 1
                    
                    # Classify by outage type
                    outage_type = outage.get('outage_type', 'utility')
                    if outage_type == 'utility':
                        utility_outages[hour] += 1
                    elif outage_type == 'internal':
                        internal_outages[hour] += 1
                    
                    # Determine if it was weekday or weekend (convert to Pakistan timezone)
                    try:
                        timestamp = datetime.fromisoformat(outage['timestamp'].replace('Z', '+00:00'))
                        # Convert to Pakistan timezone for accurate weekday/weekend determination
                        pakistan_tz = pytz.timezone('Asia/Karachi')
                        timestamp_pakistan = timestamp.astimezone(pakistan_tz)
                        is_weekend = timestamp_pakistan.weekday() >= 5  # Saturday=5, Sunday=6
                        
                        if is_weekend:
                            weekend_outages[hour] += 1
                        else:
                            weekday_outages[hour] += 1
                    except Exception as e:
                        log.warning(f"Failed to parse timestamp {outage.get('timestamp')}: {e}")
                        # Default to weekday if parsing fails
                        weekday_outages[hour] += 1
        
        # Calculate risk scores with patterns and cause classification
        total_outages = sum(hourly_outages.values())
        if total_outages > 0:
            for hour in range(24):
                count = hourly_outages[hour]
                weekday_count = weekday_outages[hour]
                weekend_count = weekend_outages[hour]
                utility_count = utility_outages[hour]
                internal_count = internal_outages[hour]
                
                # Base risk score - only utility outages affect risk buffer
                # Internal outages are handled separately for equipment diagnostics
                utility_risk_score = min(utility_count / total_outages * 2.0, 1.0)
                
                # Confidence based on total data points
                confidence = min(count / 10.0, 1.0)
                
                # Store pattern data for later use
                self.risk_profiles[hour].risk_score = utility_risk_score
                self.risk_profiles[hour].confidence = confidence
                self.risk_profiles[hour].historical_outages = count
                self.risk_profiles[hour].utility_outages = utility_count
                self.risk_profiles[hour].internal_outages = internal_count
                
                # Store weekday/weekend counts for multipliers
                self.risk_profiles[hour].weekday_outages = weekday_count
                self.risk_profiles[hour].weekend_outages = weekend_count
                
                # Log cause classification
                if utility_count > 0 or internal_count > 0:
                    log.debug(f"Hour {hour}: {utility_count} utility outages, {internal_count} internal outages")
    
    def _apply_weekday_weekend_multipliers(self):
        """Apply weekday/weekend multipliers to risk profiles."""
        for hour in range(24):
            profile = self.risk_profiles[hour]
            
            # Calculate weekday vs weekend ratios
            total_pattern_outages = getattr(profile, 'weekday_outages', 0) + getattr(profile, 'weekend_outages', 0)
            
            if total_pattern_outages > 0:
                weekday_ratio = getattr(profile, 'weekday_outages', 0) / total_pattern_outages
                weekend_ratio = getattr(profile, 'weekend_outages', 0) / total_pattern_outages
                
                # Apply multipliers based on patterns
                # If weekend has more outages, increase weekend risk
                if weekend_ratio > 0.6:  # More than 60% weekend outages
                    profile.weekend_multiplier = 1.3
                    profile.weekday_multiplier = 0.8
                elif weekday_ratio > 0.6:  # More than 60% weekday outages
                    profile.weekday_multiplier = 1.3
                    profile.weekend_multiplier = 0.8
                else:  # Balanced pattern
                    profile.weekday_multiplier = 1.0
                    profile.weekend_multiplier = 1.0
            else:
                # Default multipliers if no pattern data
                profile.weekday_multiplier = 1.0
                profile.weekend_multiplier = 1.0
    
    def _apply_seasonal_adjustments(self):
        """Apply seasonal adjustments to risk profiles."""
        from solarhub.timezone_utils import now_configured
        now = now_configured()
        month = now.month
        
        # Seasonal factors (higher risk in storm seasons)
        seasonal_factors = {
            1: 1.2,   # January - winter storms
            2: 1.1,   # February
            3: 1.0,   # March
            4: 1.0,   # April
            5: 1.0,   # May
            6: 1.0,   # June
            7: 1.0,   # July
            8: 1.0,   # August
            9: 1.1,   # September - hurricane season
            10: 1.2,  # October - hurricane season
            11: 1.1,  # November
            12: 1.2   # December - winter storms
        }
        
        factor = seasonal_factors.get(month, 1.0)
        for profile in self.risk_profiles.values():
            profile.seasonal_factor = factor
            profile.risk_score = min(profile.risk_score * factor, 1.0)
    
    def get_outage_risk(self, hour: int, is_weekend: bool = None) -> OutageRiskProfile:
        """Get outage risk profile for a specific hour with weekday/weekend multipliers."""
        base_profile = self.risk_profiles.get(hour, OutageRiskProfile(
            hour=hour, risk_score=0.1, confidence=0.5, 
            historical_outages=0, seasonal_factor=1.0,
            utility_outages=0, internal_outages=0
        ))
        
        # Apply weekday/weekend multiplier if specified
        if is_weekend is not None:
            if is_weekend:
                multiplier = getattr(base_profile, 'weekend_multiplier', 1.0)
            else:
                multiplier = getattr(base_profile, 'weekday_multiplier', 1.0)
            
            # Create adjusted profile
            adjusted_profile = OutageRiskProfile(
                hour=hour,
                risk_score=min(base_profile.risk_score * multiplier, 1.0),
                confidence=base_profile.confidence,
                historical_outages=base_profile.historical_outages,
                seasonal_factor=base_profile.seasonal_factor,
                utility_outages=base_profile.utility_outages,
                internal_outages=base_profile.internal_outages
            )
            
            return adjusted_profile
        
        return base_profile
    
    def calculate_dynamic_cushion(self, current_hour: int, forecast_uncertainty: ForecastUncertainty) -> float:
        """
        Calculate dynamic cushion above the 20% emergency reserve using historically calculated buffers.
        
        Args:
            current_hour: Current hour (0-23)
            forecast_uncertainty: Forecast uncertainty data
            
        Returns:
            Cushion percentage above 20% reserve
        """
        # Start with historically calculated base buffer
        total_cushion = self.reliability_buffer.base_buffer_pct
        
        # Add historically calculated outage risk buffer scaled by current risk
        risk_profile = self.get_outage_risk(current_hour)
        if risk_profile.risk_score > 0.3:  # High risk threshold
            risk_buffer = self.reliability_buffer.outage_risk_buffer_pct * risk_profile.risk_score
            total_cushion += risk_buffer
            log.info(f"High outage risk at hour {current_hour}: adding {risk_buffer:.1f}% buffer (historical base: {self.reliability_buffer.outage_risk_buffer_pct:.1f}%)")
        
        # Add historically calculated forecast uncertainty buffer
        if forecast_uncertainty.pv_confidence == "low":
            total_cushion += self.reliability_buffer.forecast_uncertainty_buffer_pct
            log.info(f"Low PV forecast confidence: adding {self.reliability_buffer.forecast_uncertainty_buffer_pct:.1f}% buffer")
        
        if forecast_uncertainty.load_confidence == "low":
            total_cushion += self.reliability_buffer.night_load_variability_buffer_pct
            log.info(f"Low load forecast confidence: adding {self.reliability_buffer.night_load_variability_buffer_pct:.1f}% buffer")
        
        # No hard cap - use historically calculated maximum as guidance
        # But ensure reasonable upper bound for safety
        max_reasonable = max(self.reliability_buffer.max_total_buffer_pct, 20.0)
        if total_cushion > max_reasonable:
            log.warning(f"Dynamic cushion {total_cushion:.1f}% exceeds historical maximum {self.reliability_buffer.max_total_buffer_pct:.1f}%, capping at {max_reasonable:.1f}%")
            total_cushion = max_reasonable
        
        log.info(f"Calculated dynamic cushion: {total_cushion:.1f}% above 20% reserve (historical max: {self.reliability_buffer.max_total_buffer_pct:.1f}%)")
        return total_cushion
    
    def get_effective_min_soc(self, current_hour: int, forecast_uncertainty: ForecastUncertainty) -> float:
        """
        Get the effective minimum SOC considering the 20% emergency reserve + dynamic cushion.
        
        Args:
            current_hour: Current hour (0-23)
            forecast_uncertainty: Forecast uncertainty data
            
        Returns:
            Effective minimum SOC percentage (never below 20% + cushion)
        """
        cushion = self.calculate_dynamic_cushion(current_hour, forecast_uncertainty)
        effective_min = self.emergency_reserve_pct + cushion
        
        log.info(f"Effective minimum SOC: {effective_min:.1f}% (20% reserve + {cushion:.1f}% cushion)")
        return effective_min

    # Backward-compat helper used by SmartScheduler
    def get_uncertainty_cushion_pct(self, current_hour: int = 5, forecast_uncertainty: Optional[ForecastUncertainty] = None) -> float:
        """
        Return the dynamic cushion percentage used in scheduling decisions.
        Defaults to a predawn hour (5) with medium forecast assumptions when not provided.
        """
        if forecast_uncertainty is None:
            forecast_uncertainty = ForecastUncertainty(
                pv_confidence="medium",
                pv_uncertainty_pct=5.0,
                load_confidence="medium",
                load_uncertainty_pct=5.0,
                pv_p75=0.0,
                pv_p90=0.0,
                load_p75=0.0,
                load_p90=0.0,
            )
        return self.calculate_dynamic_cushion(current_hour, forecast_uncertainty)
    
    def assess_forecast_uncertainty(self, pv_forecast: List[float], load_forecast: List[float]) -> ForecastUncertainty:
        """
        Assess forecast uncertainty based on historical accuracy and forecast characteristics.
        
        Args:
            pv_forecast: PV forecast values
            load_forecast: Load forecast values
            
        Returns:
            Forecast uncertainty assessment
        """
        if not pv_forecast or not load_forecast:
            return ForecastUncertainty("low", 20.0, "low", 20.0, 0, 0, 0, 0)
        
        # Calculate forecast statistics
        pv_mean = sum(pv_forecast) / len(pv_forecast)
        load_mean = sum(load_forecast) / len(load_forecast)
        
        # Calculate variance and coefficient of variation
        pv_variance = sum((x - pv_mean)**2 for x in pv_forecast) / len(pv_forecast)
        load_variance = sum((x - load_mean)**2 for x in load_forecast) / len(load_forecast)
        
        pv_cv = math.sqrt(pv_variance) / pv_mean if pv_mean > 0 else 1.0
        load_cv = math.sqrt(load_variance) / load_mean if load_mean > 0 else 1.0
        
        # Assess PV confidence based on multiple factors
        pv_confidence = self._assess_pv_confidence(pv_forecast, pv_cv)
        load_confidence = self._assess_load_confidence(load_forecast, load_cv)
        
        # Calculate uncertainty percentages
        pv_uncertainty_pct = min(pv_cv * 100, 50.0)  # Cap at 50%
        load_uncertainty_pct = min(load_cv * 100, 30.0)  # Cap at 30%
        
        # Calculate percentile bands
        pv_sorted = sorted(pv_forecast)
        load_sorted = sorted(load_forecast)
        
        pv_p75 = pv_sorted[int(len(pv_sorted) * 0.75)] if pv_sorted else 0
        pv_p90 = pv_sorted[int(len(pv_sorted) * 0.90)] if pv_sorted else 0
        load_p75 = load_sorted[int(len(load_sorted) * 0.75)] if load_sorted else 0
        load_p90 = load_sorted[int(len(load_sorted) * 0.90)] if load_sorted else 0
        
        return ForecastUncertainty(
            pv_confidence=pv_confidence,
            pv_uncertainty_pct=pv_uncertainty_pct,
            load_confidence=load_confidence,
            load_uncertainty_pct=load_uncertainty_pct,
            pv_p75=pv_p75,
            pv_p90=pv_p90,
            load_p75=load_p75,
            load_p90=load_p90
        )
    
    def _assess_pv_confidence(self, pv_forecast: List[float], cv: float) -> str:
        """Assess PV forecast confidence based on forecast characteristics."""
        # Check for zero or very low values (indicating poor conditions)
        zero_count = sum(1 for x in pv_forecast if x < 0.1)
        zero_ratio = zero_count / len(pv_forecast)
        
        # Check for extreme variability
        max_pv = max(pv_forecast) if pv_forecast else 0
        min_pv = min(pv_forecast) if pv_forecast else 0
        range_ratio = (max_pv - min_pv) / max_pv if max_pv > 0 else 1.0
        
        # Determine confidence level
        if zero_ratio > 0.3 or cv > 0.8 or range_ratio > 0.9:
            return "low"
        elif zero_ratio > 0.1 or cv > 0.4 or range_ratio > 0.6:
            return "medium"
        else:
            return "high"
    
    def _assess_load_confidence(self, load_forecast: List[float], cv: float) -> str:
        """Assess load forecast confidence based on forecast characteristics."""
        # Check for extreme variability in load
        max_load = max(load_forecast) if load_forecast else 0
        min_load = min(load_forecast) if load_forecast else 0
        range_ratio = (max_load - min_load) / max_load if max_load > 0 else 0
        
        # Check for unusual patterns (very high or very low loads)
        mean_load = sum(load_forecast) / len(load_forecast)
        extreme_count = sum(1 for x in load_forecast if x > mean_load * 2 or x < mean_load * 0.1)
        extreme_ratio = extreme_count / len(load_forecast)
        
        # Determine confidence level
        if cv > 0.6 or range_ratio > 0.8 or extreme_ratio > 0.2:
            return "low"
        elif cv > 0.3 or range_ratio > 0.5 or extreme_ratio > 0.1:
            return "medium"
        else:
            return "high"
    
    def check_presunset_assurance(self, current_soc_pct: float, sunset_soc_projection: float, 
                                night_load_estimate: float, battery_capacity_kwh: float) -> bool:
        """
        Check if pre-sunset assurance is needed.
        
        Args:
            current_soc_pct: Current SOC percentage
            sunset_soc_projection: Projected SOC at sunset
            night_load_estimate: Estimated night load in kWh
            battery_capacity_kwh: Battery capacity in kWh
            
        Returns:
            True if pre-sunset top-up is needed
        """
        # Calculate required SOC for night load + buffers + 20% reserve
        night_load_soc = (night_load_estimate / battery_capacity_kwh) * 100
        required_soc = night_load_soc + self.calculate_dynamic_cushion(18, ForecastUncertainty("medium", 5.0, "medium", 5.0, 0, 0, 0, 0)) + self.emergency_reserve_pct
        # Never target above 100%; cap and warn if impossible requirement occurs
        if required_soc > 100.0:
            log.warning(f"Required SOC exceeds 100% (computed {required_soc:.1f}%). Capping to 100%. Consider increasing battery capacity or allowing costlier pre-dawn windows.")
            required_soc = 100.0

        # Apply a small hysteresis to avoid oscillation when we're already very close
        # Only force if the projection is below the requirement by a meaningful margin
        hysteresis_pct = 2.0  # percent points
        if sunset_soc_projection + hysteresis_pct < required_soc:
            log.warning(f"Pre-sunset assurance needed: projected {sunset_soc_projection:.1f}% < required {required_soc:.1f}% (hysteresis {hysteresis_pct:.1f}%)")
            return True

        # If requirement is capped at 100% and we're within hysteresis (e.g., 98-99%), suppress forcing
        if required_soc >= 100.0:
            log.info(
                f"Pre-sunset assurance suppressed by hysteresis: projected {sunset_soc_projection:.1f}%  required {required_soc:.1f}%"
            )
        return False
    
    def check_no_outage_guardrails(self, current_soc_pct: float, projected_night_soc: float,
                                 night_load_kwh: float, battery_capacity_kwh: float,
                                 cheap_windows_available: bool, current_hour: int) -> Dict[str, Any]:
        """
        Check explicit no-outage guardrails with proactive alerts.
        
        Args:
            current_soc_pct: Current SOC percentage
            projected_night_soc: Projected SOC after night load
            night_load_kwh: Expected night load in kWh
            battery_capacity_kwh: Battery capacity in kWh
            cheap_windows_available: Whether cheap pre-dawn windows are available
            current_hour: Current hour (0-23)
            
        Returns:
            Dictionary with guardrail status and recommendations
        """
        # Calculate effective minimum SOC (20% + dynamic cushion)
        effective_min_soc = self.get_effective_min_soc(
            current_hour,
            ForecastUncertainty(pv_confidence="medium", pv_uncertainty_pct=5.0, 
                              load_confidence="medium", load_uncertainty_pct=5.0,
                              pv_p75=0, pv_p90=0, load_p75=0, load_p90=0)
        )
        
        # Calculate projected SOC after night load
        soc_after_night = projected_night_soc - (night_load_kwh / battery_capacity_kwh) * 100
        
        # Check if we're at risk of going below emergency reserve
        emergency_reserve_breach = soc_after_night < 20.0
        effective_min_breach = soc_after_night < effective_min_soc
        
        result = {
            "emergency_reserve_breach": emergency_reserve_breach,
            "effective_min_breach": effective_min_breach,
            "projected_soc_after_night": soc_after_night,
            "effective_min_soc": effective_min_soc,
            "emergency_reserve": 20.0,
            "alert_level": "none",
            "recommendations": [],
            "allow_costlier_windows": False
        }
        
        if emergency_reserve_breach:
            # CRITICAL: Risk of going below 20% emergency reserve
            result["alert_level"] = "critical"
            result["recommendations"].append("CRITICAL: Projected SOC will breach 20% emergency reserve")
            result["recommendations"].append("IMMEDIATE ACTION: Allow costlier grid charging windows")
            result["allow_costlier_windows"] = True
            
            log.critical(f"NO-OUTAGE GUARDRAIL BREACH: Projected SOC {soc_after_night:.1f}% < 20% emergency reserve")
            log.critical(f"ALLOWING COSTLIER WINDOWS to prevent blackout")
            
        elif effective_min_breach:
            # WARNING: Risk of going below effective minimum
            result["alert_level"] = "warning"
            result["recommendations"].append(f"WARNING: Projected SOC {soc_after_night:.1f}% < effective minimum {effective_min_soc:.1f}%")
            
            if cheap_windows_available:
                result["recommendations"].append("Use available cheap pre-dawn windows")
            else:
                result["recommendations"].append("No cheap windows available - consider costlier options")
                result["allow_costlier_windows"] = True
            
            log.warning(f"NO-OUTAGE GUARDRAIL WARNING: Projected SOC {soc_after_night:.1f}% < effective minimum {effective_min_soc:.1f}%")
            
        else:
            # SAFE: Within acceptable limits
            result["alert_level"] = "safe"
            result["recommendations"].append(f"SAFE: Projected SOC {soc_after_night:.1f}% above effective minimum {effective_min_soc:.1f}%")
            log.info(f"NO-OUTAGE GUARDRAIL: Projected SOC {soc_after_night:.1f}% is safe")
        
        return result
    
    def check_predawn_insurance(self, current_soc_pct: float, predawn_soc_projection: float) -> bool:
        """
        Check if pre-dawn insurance top-up is needed.
        
        Args:
            current_soc_pct: Current SOC percentage
            predawn_soc_projection: Projected SOC at 5:30 AM
            
        Returns:
            True if pre-dawn top-up is needed
        """
        # Target SOC at 5:30 AM should be well above 20% + cushion
        target_soc = self.emergency_reserve_pct + self.calculate_dynamic_cushion(5, ForecastUncertainty("medium", 5.0, "medium", 5.0, 0, 0, 0, 0)) + 5.0  # Extra 5% for morning buffer
        if target_soc > 100.0:
            log.warning(f"Pre-dawn target SOC exceeds 100% (computed {target_soc:.1f}%). Capping to 100%.")
            target_soc = 100.0
        
        if predawn_soc_projection < target_soc:
            log.warning(f"Pre-dawn insurance needed: projected {predawn_soc_projection:.1f}% < target {target_soc:.1f}%")
            return True
        
        return False
    
    def get_predawn_insurance_window(self, tznow: Optional[pd.Timestamp] = None, sunset_calc=None) -> Tuple[str, str]:
        """
        Get a narrow 1-hour pre-dawn insurance window centered around ~04:00 local time,
        seasonally adjusted using sunrise. Defaults to 03:3004:30 PKT if sunrise unavailable.
        
        Args:
            tznow: current localized timestamp
            sunset_calc: calculator providing get_sunrise_hour
        Returns:
            (start_time, end_time) as HH:MM strings
        """
        try:
            if tznow is None:
                from datetime import datetime, timezone
                from solarhub.timezone_utils import now_configured
                tznow = pd.Timestamp(now_configured()).tz_localize(None)
            if sunset_calc is not None:
                sunrise_h = int(sunset_calc.get_sunrise_hour(tznow))
                # Center 1 hour ending ~30m before sunrise, e.g., [sunrise-1:30, sunrise-0:30]
                center = max(1, sunrise_h) - 1  # one hour before sunrise
                start_h = max(0, center)
                start = f"{start_h:02d}:30"
                end_h = start_h + 1
                end = f"{end_h:02d}:30"
                return (start, end)
        except Exception:
            pass
        # Fallback to 03:3004:30
        return ("03:30", "04:30")
    
    def should_activate_predawn_insurance(self, current_hour: int, current_soc_pct: float, 
                                        forecast_uncertainty: ForecastUncertainty) -> bool:
        """
        Determine if pre-dawn insurance should be activated.
        
        Args:
            current_hour: Current hour (0-23)
            current_soc_pct: Current SOC percentage
            forecast_uncertainty: Forecast uncertainty data
            
        Returns:
            True if pre-dawn insurance should be activated
        """
        # Only activate during pre-dawn hours (00:00-06:00)
        if not (0 <= current_hour <= 6):
            return False
        
        # Check if we're in a high-risk period
        risk_profile = self.get_outage_risk(current_hour)
        if risk_profile.risk_score > 0.4:  # High risk threshold
            log.info(f"High outage risk at hour {current_hour} - activating pre-dawn insurance")
            return True
        
        # Check if forecast uncertainty is high
        if forecast_uncertainty.pv_confidence == "low" or forecast_uncertainty.load_confidence == "low":
            log.info(f"High forecast uncertainty - activating pre-dawn insurance")
            return True
        
        # Check if SOC is approaching the effective minimum
        effective_min = self.get_effective_min_soc(current_hour, forecast_uncertainty)
        if current_soc_pct < effective_min + 5.0:  # Within 5% of effective minimum
            log.info(f"SOC approaching effective minimum - activating pre-dawn insurance")
            return True
        
        return False
    
    def record_outage_event(self, timestamp: datetime, duration_minutes: int, cause: str = "unknown", 
                          outage_type: str = "utility"):
        """
        Record an outage event for future risk analysis with cause classification.
        
        Args:
            timestamp: When the outage occurred
            duration_minutes: How long the outage lasted
            cause: Cause of the outage (storm, maintenance, equipment_failure, etc.)
            outage_type: Type of outage - "utility" or "internal"
        """
        outage_event = {
            "timestamp": timestamp.isoformat(),
            "hour": timestamp.hour,
            "duration_minutes": duration_minutes,
            "cause": cause,
            "outage_type": outage_type
        }
        
        self.outage_history.append(outage_event)
        
        # Keep only last 30 days
        from solarhub.timezone_utils import now_configured
        cutoff_date = now_configured() - timedelta(days=30)
        self.outage_history = [
            event for event in self.outage_history
            if datetime.fromisoformat(event["timestamp"].replace('Z', '+00:00')) > cutoff_date
        ]
        
        self._save_outage_history()
        log.info(f"Recorded {outage_type} outage event: {duration_minutes}min at hour {timestamp.hour}, cause: {cause}")
    
    def classify_outage_cause(self, telemetry: Dict[str, Any], duration_minutes: int) -> Tuple[str, str]:
        """
        Classify outage cause based on telemetry data and duration.
        
        Args:
            telemetry: Current telemetry data
            duration_minutes: Duration of the outage
            
        Returns:
            Tuple of (outage_type, cause) where outage_type is "utility" or "internal"
        """
        # Check for internal protection trip indicators
        if isinstance(telemetry, dict):
            inverter_mode = telemetry.get('inverter_mode')
            grid_voltage = telemetry.get('grid_voltage', 0)
            grid_frequency = telemetry.get('grid_frequency_hz') or telemetry.get('grid_frequency', 0)
            battery_voltage = telemetry.get('battery_voltage_v') or telemetry.get('batt_voltage_v') or telemetry.get('battery_voltage', 0)
            battery_current = telemetry.get('battery_current_a') or telemetry.get('batt_current_a') or telemetry.get('battery_current', 0)
        else:
            # Telemetry object
            inverter_mode = getattr(telemetry, 'inverter_mode', None)
            if hasattr(telemetry, 'extra') and telemetry.extra:
                inverter_mode = telemetry.extra.get('inverter_mode', inverter_mode)
            grid_voltage = getattr(telemetry, 'grid_voltage', 0)
            grid_frequency = getattr(telemetry, 'grid_frequency', 0)
            battery_voltage = getattr(telemetry, 'batt_voltage_v', 0)
            battery_current = getattr(telemetry, 'batt_current_a', 0)
        
        # Internal protection trip indicators
        if (inverter_mode == 0x04 and  # OffGrid mode
            grid_voltage > 200 and  # Grid voltage is normal
            grid_frequency >= 49.5 and grid_frequency <= 50.5):  # Grid frequency is normal
            # Grid is available but inverter switched to off-grid - likely internal protection
            if duration_minutes < 5:
                return "internal", "short_protection_trip"
            else:
                return "internal", "extended_protection_trip"
        
        # Battery-related internal issues
        if battery_voltage < 40 or battery_current > 100:  # Abnormal battery conditions
            return "internal", "battery_protection_trip"
        
        # Temperature-related internal issues
        if isinstance(telemetry, dict):
            inverter_temp = telemetry.get('inverter_temp_c', 0)
        else:
            # Telemetry object
            inverter_temp = getattr(telemetry, 'inverter_temp_c', 0)
            if hasattr(telemetry, 'extra') and telemetry.extra:
                inverter_temp = telemetry.extra.get('inverter_temp_c', inverter_temp)
        
        if inverter_temp > 60:  # High temperature
            return "internal", "thermal_protection_trip"
        
        # Default to utility outage for external causes
        if duration_minutes < 2:
            return "utility", "brief_grid_flicker"
        elif duration_minutes < 30:
            return "utility", "short_grid_outage"
        elif duration_minutes < 120:
            return "utility", "extended_grid_outage"
        else:
            return "utility", "major_grid_outage"
    
    def get_equipment_diagnostics(self, current_hour: int) -> Dict[str, Any]:
        """
        Get equipment diagnostics based on internal outage patterns.
        
        Args:
            current_hour: Current hour (0-23)
            
        Returns:
            Dictionary with equipment diagnostic information
        """
        profile = self.risk_profiles.get(current_hour, OutageRiskProfile(
            hour=current_hour, risk_score=0.1, confidence=0.5, 
            historical_outages=0, seasonal_factor=1.0, utility_outages=0, internal_outages=0
        ))
        
        diagnostics = {
            "hour": current_hour,
            "internal_outages": profile.internal_outages,
            "utility_outages": profile.utility_outages,
            "equipment_health": "good",
            "recommendations": []
        }
        
        # Analyze internal outage patterns
        if profile.internal_outages > 0:
            if profile.internal_outages >= 3:
                diagnostics["equipment_health"] = "poor"
                diagnostics["recommendations"].append("High frequency of internal protection trips - equipment inspection needed")
            elif profile.internal_outages >= 2:
                diagnostics["equipment_health"] = "fair"
                diagnostics["recommendations"].append("Moderate internal protection trips - monitor equipment closely")
            else:
                diagnostics["equipment_health"] = "good"
                diagnostics["recommendations"].append("Occasional internal protection trips - normal operation")
        
        # Compare utility vs internal outage ratio
        total_outages = profile.utility_outages + profile.internal_outages
        if total_outages > 0:
            internal_ratio = profile.internal_outages / total_outages
            if internal_ratio > 0.5:
                diagnostics["recommendations"].append("High internal outage ratio - check equipment configuration and protection settings")
            elif internal_ratio > 0.3:
                diagnostics["recommendations"].append("Moderate internal outage ratio - review protection settings")
        
        return diagnostics
    
    def check_grid_instability(self, telemetry: Dict[str, Any], current_hour: int) -> bool:
        """
        Check for grid instability during historically risky hours.
        
        Args:
            telemetry: Current telemetry data
            current_hour: Current hour (0-23)
            
        Returns:
            True if grid instability is detected
        """
        risk_profile = self.get_outage_risk(current_hour)
        
        # Only check during high-risk hours
        if risk_profile.risk_score < 0.3:
            return False
        
        # Primary: use inverter_mode to distinguish absence vs instability
        if isinstance(telemetry, dict):
            inverter_mode = telemetry.get("inverter_mode")
            grid_power = telemetry.get("grid_power_w")
            grid_voltage = telemetry.get("grid_voltage", 0)
            grid_frequency = telemetry.get("grid_frequency", 0)
        else:
            # Telemetry object
            inverter_mode = getattr(telemetry, 'inverter_mode', None)
            if hasattr(telemetry, 'extra') and telemetry.extra:
                inverter_mode = telemetry.extra.get('inverter_mode', inverter_mode)
            grid_power = getattr(telemetry, 'grid_power_w', None)
            grid_voltage = getattr(telemetry, 'grid_voltage', 0)
            grid_frequency = getattr(telemetry, 'grid_frequency', 0)
        
        mode_str = None
        if isinstance(inverter_mode, str):
            mode_str = inverter_mode
        elif isinstance(inverter_mode, (int, float)):
            if inverter_mode == 0x03 or inverter_mode == 3:
                mode_str = "OnGrid mode"
            elif inverter_mode == 0x04 or inverter_mode == 4:
                mode_str = "OffGrid mode"

        # If explicitly OffGrid, treat as grid absent (not instability)
        if mode_str and "OffGrid" in mode_str:
            return False

        # Check for grid power anomalies only when inverter reports OnGrid (or mode unknown)
        # Use broader set of keys seen in telemetry
        if grid_power is None:
            if isinstance(telemetry, dict):
                for key in ["grid_power", "grid_watt", "phase_r_watt_of_grid", "phase_s_watt_of_grid", "phase_t_watt_of_grid"]:
                    if key in telemetry:
                        try:
                            grid_power = float(telemetry[key])
                            break
                        except Exception:
                            continue
            else:
                # For Telemetry objects, check extra field
                if hasattr(telemetry, 'extra') and telemetry.extra:
                    for key in ["grid_power", "grid_watt", "phase_r_watt_of_grid", "phase_s_watt_of_grid", "phase_t_watt_of_grid"]:
                        if key in telemetry.extra:
                            try:
                                grid_power = float(telemetry.extra[key])
                                break
                            except Exception:
                                continue
        if grid_power is None:
            grid_power = 0.0

        # Only treat very low grid power as instability if we are OnGrid or mode unknown
        if (not mode_str or "OnGrid" in mode_str) and grid_power < 10:
            log.warning(f"Grid instability detected: very low grid power {grid_power}W during high-risk hour {current_hour}")
            return True
        
        # Grid voltage instability (outside normal range)
        if grid_voltage > 0 and (grid_voltage < 200 or grid_voltage > 250):  # Outside normal range
            log.warning(f"Grid instability detected: abnormal voltage {grid_voltage}V during high-risk hour {current_hour}")
            return True
        
        # Grid frequency instability (outside normal range)
        if grid_frequency > 0 and (grid_frequency < 49.5 or grid_frequency > 50.5):  # Outside normal range
            log.warning(f"Grid instability detected: abnormal frequency {grid_frequency}Hz during high-risk hour {current_hour}")
            return True
        
        return False
    
    def get_reliability_status(self) -> Dict[str, Any]:
        """Get current reliability status for monitoring."""
        from solarhub.timezone_utils import now_configured
        current_hour = now_configured().hour
        risk_profile = self.get_outage_risk(current_hour)
        
        return {
            "current_hour": current_hour,
            "outage_risk_score": risk_profile.risk_score,
            "outage_risk_confidence": risk_profile.confidence,
            "emergency_reserve_pct": self.emergency_reserve_pct,
            "dynamic_cushion_pct": self.calculate_dynamic_cushion(current_hour, ForecastUncertainty("medium", 5.0, "medium", 5.0, 0, 0, 0, 0)),
            "effective_min_soc_pct": self.get_effective_min_soc(current_hour, ForecastUncertainty("medium", 5.0, "medium", 5.0, 0, 0, 0, 0)),
            "total_outage_events": len(self.outage_history),
            "recent_outages_24h": len([e for e in self.outage_history if datetime.fromisoformat(e["timestamp"].replace('Z', '+00:00')).replace(tzinfo=timezone.utc) > now_configured() - timedelta(hours=24)])
        }
